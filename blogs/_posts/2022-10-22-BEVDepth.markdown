---
layout: post
title: BEVDepth&#58; Acquisition of Reliable Depth for Multi-view 3D Object Detection
image: tiv.jpg
date: 2022-10-18 15:52:00 +0800
tags: [Computer Vision, 3D Object Detection]
categories: papers
---


## Contributions
1. Propose BEVDepth, a multi-view 3D object detector with explicit depth supervision, which simultaneously encodes the camera parameters and disturbance.
2. Design a new view-transform operation (namely Efficient Voxel Pooling) to consume the depth, which significantly reduces the time cost.
3. Evaluate the proposed BEVDepth on the challenging benchmark nuScenes and achieve state-of-the-art while maintaining the high efficiency.

<br/>

## Depth Analysis

![](https://github.com/Zanue/Zanue.github.io/raw/main/images/blog_img/bevdepth/bevdepth-depth.jpg){:height="80%" width="80%"}  

<br/>

## Architecture

![](https://github.com/Zanue/Zanue.github.io/raw/main/images/blog_img/bevdepth/bevdepth-framework.jpg){:height="80%" width="80%"}  


A vanilla BEVDepth simply replaces the segmentation head in LSS [1] with CenterPoint [2] head for 3D detection.

### Explicit Depth Supervision

Supervise the intermediate depth prediction $D^{pred}$ using ground-truth $D^{gt}$ derived from point clouds data $P$.

1. Use rotation and translation matrix from the ego coordinate to the camera coordinate to calculate 2.5D image coordinates $P_ {i}^{img} (u, v, d)$;
2. To align the shape between the projected point clouds and the predicted depth, a *min pooling* and a *one hot* are adopted on $P_ {i}^{img}$;
3. As for the depth loss $L_ {depth}, simply adopt Binary Cross Entropy.

### Depth Correction

Motivation: The calibrated camera parameters $R$ and $\mathbf{t}$ may be imprecise at times due to the ego vehicle’s vigorous movement, resulting in spatial mis-alignment between $F^{2d}$ and $D^{gt}$.

Solution: Stack multiple Residual Blocks in DepthNet to increase the receptive field, followed by a Deformable Conv.

### Camera-aware Depth Prediction

![](https://github.com/Zanue/Zanue.github.io/raw/main/images/blog_img/bevdepth/bevdepth-fig34.jpg){:height="80%" width="80%"}  


Motivation: Estimating pixel depth is associated to the camera intrinsics. This is especially important in multi-view3D datasets when cameras may have different FOVs (e.g., nuScenes Dataset).

Solution: Utilize the camera intrinsics as one of the inputs for DepthNet.

1. Camera intrinsics’ dimension is first scaled up to the feature’s using an MLP layer;
2. Then, they are used to re-weight the image feature $F^{2d}_ i$ with an Squeeze-and-Excitation [3] module;
3. Finally, concatenate the camera extrinsics to it’s intrinsics to help DepthNet aware of $F^{2d}$’s spatial location in the ego coordinate system.

Denote $\psi$ as the original DepthNet, the overall Camera-awareness depth prediction can be written in:

<p>
$$
D^{pred}_ i = \psi (SE(F^{2d}_ i | MLP(\xi (R_ i) \oplus \xi (\mathbf{t}_ i) \oplus \xi (K_ i)))),
$$
</p>

where $\xi$ denotes the Flatten operation.

### Efficient Voxel Pooling

Motivation: Efficiency.

Solution: Assign each frustum feature a CUDA thread which is used to add that feature to its corresponding BEV grid. Replacing the original Voxel Pooling operation with the improved version can speed up BEVDepth up to $\times 3$.

<br/>


## Experiments

### Ablation Study

![](https://github.com/Zanue/Zanue.github.io/raw/main/images/blog_img/bevdepth/bevdepth-tab12.jpg){:height="80%" width="80%"}  


![](https://github.com/Zanue/Zanue.github.io/raw/main/images/blog_img/bevdepth/bevdepth-fig5tab3.jpg){:height="80%" width="80%"}  


### Multi-frame Fusion

![](https://github.com/Zanue/Zanue.github.io/raw/main/images/blog_img/bevdepth/bevdepth-fusion.jpg){:height="80%" width="80%"}  


### Compare with SOTA

![](https://github.com/Zanue/Zanue.github.io/raw/main/images/blog_img/bevdepth/bevdepth-comparison.jpg){:height="80%" width="80%"}  


<br/>

## References
[1] Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d. In European Conference on Computer Vision, pages 194–210. Springer, 2020.

[2] Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-based 3d object detection and tracking. In Proceedings ofthe IEEE/CVF conference on computer vision and pattern recognition, pages 11784–11793, 2021.

[3] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings ofthe IEEE conference on computer vision and pattern recognition, pages 7132–7141, 2018.