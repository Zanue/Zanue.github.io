---
layout: post
title: RangeDet&#58; In Defense of Range View for LiDAR-based 3D Object Detection
image: Dana2.jpg
date: 2022-10-14 15:38:00 +0800
tags: [Computer Vision, 3D Object Detection]
categories: papers
---



## Core Novelty
method is purely based on the range view representation

<br/>


## Background

Advantages and disadvantages of Range-view:

1. The range view is widely adopted in semantic segmentation task, but it is rarely used in object detection task individually.
2. Organizing the point cloud in range view misses no information.
3. The compactness also enables fast neighborhood queries based on range image coordinates, while point view methods usually need a time-consuming ball query algorithm to get the neighbors.
4. The valid detection range of range-view-based detectors can be as far as the sensor’s availability, while we have to set a threshold for the detection range in BEV-based 3D detectors.


However, there is still a huge gap between the pure range-view-based method and the BEV-based method. For example, on the validation split of Waymo Open Dataset (WOD), *they are still lower than state-of-the-art methods by a large margin (more than 20 points 3D AP in vehicle class)*.


Some facts on the different designs of RV and BEV:
1. **scale variation**: the challenge of detecting objects with sparse points in BEV is converted to the challenge of scale variation in the range image.
2. **input 2D output 3D**: unlike in 2D image, though the convolution on range image is conducted on 2D pixel coordinates, while the output is in the 3D space.
3. **compactness**: the 2D range view is naturally more compact than 3D space, which makes feature extractions in range-image-based detectors more efficient.

Corresponding solutions:
1. **Range Conditioned Pyramid**
2. **Meta-Kernel**
3. **Weighted Non-Maximum Suppression**

<br/>

## Review of Range View Representation

![](https://github.com/Zanue/Zanue.github.io/raw/main/images/blog_img/rangedet/rangedet-lidar.jpg){:height="40%" width="40%"}  
*The illustration of the native range image.*

![](https://github.com/Zanue/Zanue.github.io/raw/main/images/blog_img/rangedet/rangedet-wod.jpg){:height="40%" width="40%"}  
*Waymo Open Dataset: properties of each pixel in the range image.*


## Methodology

### Range Conditioned Pyramid
- In the original FPN, the ground-truth bounding box is assigned based on its area in the 2D image.
- RCP designate the objects with a similar range to be processed by the same layer instead of purely using the area in FPN.

### Meta-Kernel Convolution
![](https://github.com/Zanue/Zanue.github.io/raw/main/images/blog_img/rangedet/rangedet-metakernel.jpg){:height="80%" width="80%"}  
*The illustration of Meta-Kernel*

1. **For weight acquisition**, relationship vector is $\mathbf{h}(\mathbf{p}_0, \mathbf{p}_n)$, convolution weight is $\mathbf{W}_{\mathbf{p}_0}(\mathbf{p}_n)$, we have:  
   
    $$
    \mathbf{W}_{\mathbf{p}_0}(\mathbf{p}_n) = \mathrm{MLP}(\mathbf{h}(\mathbf{p}_0, \mathbf{p}_n))
    $$  
2. **For multiplication**, instead of matrix multiplication, simply use element-wise product:  
   
    $$
    \mathbf{o}_{\mathbf{p}_0}(\mathbf{p}_n) = \mathbf{W}_{\mathbf{p}_0}(\mathbf{p}_n) \odot \mathbf{F}(\mathbf{p}_0 + \mathbf{p}_n)
    $$  
3. **For aggregation**, instead of channel-wise summation, concatenate all $\mathbf{o}_{\mathbf{p}_0}(\mathbf{p}_n), \forall \mathbf{p}_n \in \mathcal{G}$ and pass it to a fully-connected layer.

Summing it up, the Meta-Kernel can be formulated as:  

$$
\mathbf{z}(\mathbf{p}_0) = \mathcal{A}(\mathbf{W}_{\mathbf{p}_0}(\mathbf{p}_n) \odot \mathbf{F}(\mathbf{p}_0 + \mathbf{p}_n)), \forall \mathbf{p}_n \in \mathcal{G}
$$

## Weighted Non-Maximum Suppression
1. filter out the proposals whose scores are less than a predefined threshold 0.5
2. For the current top-rank proposal $\mathbf{b}_0$, find the proposals whose IoUs with $\mathbf{b}_0$ are higher than 0.5
3. The output bounding box for $\mathbf{b}_0$ is a weighted average of these proposals, which can be described as:  
   
   $$
   \hat{\mathbf{b}}_0 = \frac{\sum_k \mathbb{I}(\mathrm{IoU}(\mathbf{b}_0, \mathbf{b}_k) > t) s_k \mathbf{b}_k}{\sum_k \mathbb{I}(\mathrm{IoU}(\mathbf{b}_0, \mathbf{b}_k) > t) s_k}
   $$  

   where $\mathbf{b}_k$ and $s_k$ denote other proposals and corresponding scores. $t$ is the IoU threshold, which is 0.5. $\mathbb{I}$(·) is the indicator function.

## Data Augmentation in Range View
*Random global rotation*, *Random global flip* and *Copy-Paste*


### Architecture
![](https://github.com/Zanue/Zanue.github.io/raw/main/images/blog_img/rangedet/rangedet-framework.jpg){:height="80%" width="80%"}  
*The overall architecture of RangeDet*

- The eight input range image channels include *range, intensity, elongation, x, y, z, azimuth, and inclination*
- Meta-Kernel is *(only?)* placed in the second BasicBlock
- In WOD, the range of a point varies from 0m to 80m. Divide [0, 80] to 3 intervals: [0, 15), [15, 30), [30, 80]
- For point $i$, regard its azimuth direction as its local x-axis which is the same as in LaserNet
- classification loss:  
  
  $$
   L_{cls} = \frac{1}{M} \sum_{i} \mathrm{VFL}_i
  $$  
  
  $\mathrm{VFL}_i$ is the varifocal loss of each point:

  ![](https://github.com/Zanue/Zanue.github.io/raw/main/images/blog_img/rangedet/rangedet-vfl.jpg){:height="40%" width="40%"}
- regression loss: <br/>
  ![](https://github.com/Zanue/Zanue.github.io/raw/main/images/blog_img/rangedet/rangedet-regloss1.jpg){:height="40%" width="40%"}  

  ![](https://github.com/Zanue/Zanue.github.io/raw/main/images/blog_img/rangedet/rangedet-regloss2.jpg){:height="40%" width="40%"}



## Experiments

### Ablation
![](https://github.com/Zanue/Zanue.github.io/raw/main/images/blog_img/rangedet/rangedet-ablation.jpg){:height="80%" width="80%"}

### Compare with SOTA
![](https://github.com/Zanue/Zanue.github.io/raw/main/images/blog_img/rangedet/rangedet-sotacomp.jpg){:height="80%" width="80%"}



